from torch import cuda, bfloat16
import transformers

device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'

from langchain.document_loaders import WebBaseLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.vectorstores import FAISS

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

from langchain_openai import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain

embedding_function=HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
llm = ChatOpenAI(temperature=0.0, base_url="https://direct-piranha-hideously.ngrok-free.app/v1/", api_key="not-needed")


#---------Part 1

web_links = ["https://google.com"]

loader = WebBaseLoader(web_links)
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)
all_splits = text_splitter.split_documents(documents)

vectorstore = FAISS.from_documents(all_splits, embedding_function)



chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)

chat_history = []

query1 = "Truong My Lan bi bat"
result1 = chain({"question": query1, "chat_history": chat_history})

print(result1['answer'])

chat_history = [(query1, result1["answer"])]

print(result1['source_documents'])

#---------Part 2

vector_db = Chroma(persist_directory="./chroma_db_nccn", embedding_function=embedding_function)

query2 = ""

print("\n\nSearching for similar documents to:", query2)

search_results = vector_db.similarity_search(query2, k=2)

# make a string of the search results
search_results_string = ""
for result in search_results:
    search_results_string += result.page_content + "\n\n"

# print the string
#print(search_results_string)


# Build prompt
from langchain.prompts import PromptTemplate
template = """Use the following pieces of context to answer the question at the end. \
    If you don't know the answer, just say that you don't know, don't try to make up an answer. \
    Use three sentences maximum. Keep the answer as concise as possible. Always say \
    "thanks for asking!" at the end of the answer.  {context} \
    Question: {question}
    Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"],template=template,)

# Run chain
from langchain.chains import RetrievalQA
question = ""
qa_chain = RetrievalQA.from_chain_type(llm,
                                        retriever=vector_db.as_retriever(),
                                        return_source_documents=True,
                                        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT})
