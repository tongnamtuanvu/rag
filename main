import streamlit as st
from torch import cuda
import torch
import langchain

from langchain.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain

from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
embedding_function = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
llm = ChatOpenAI(temperature=0.0, base_url="https://guided-optionally-rhino.ngrok-free.app/v1/", api_key="not-needed")
# Part 1
def part1():
    st.subheader("Access Data realtime")
    # Input fields
    web_links = st.text_input("Enter Web links (separated by , )", value="https://google.com")
    query1 = st.text_input("Enter your prompt", value="truong my lan bi bat")
    if st.button("Perform"):
        # Your Part 1 code here
        web_links_list = [link.strip() for link in web_links.split(",")]
        loader = WebBaseLoader(web_links_list)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)
        all_splits = text_splitter.split_documents
        vectorstore = FAISS.from_documents(all_splits, embedding_function)
        chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)
        chat_history = []
        result1 = chain({"question": query1, "chat_history": chat_history})
        st.write(result1['answer'])
        chat_history = [(query1, result1["answer"])]
        st.write(result1['source_documents'])

# Part 2
def part2():
    st.subheader("Access Data local")
    # Input field
    query2 = st.text_input("Enter your prompts", value="")
    if st.button("Run"):
        # Your Part 2 code here
        vector_db = Chroma(persist_directory="./chroma_db_nccn", embedding_function=embedding_function)
        search_results = vector_db.similarity_search(query2, k=2)
        search_results_string = ""
        for result in search_results:
            search_results_string += result.page_content + "\n\n"
        st.write(search_results_string)
        template = """Use the following pieces of context to answer the question at the end. \
            If you don't know the answer, just say that you don't know, don't try to make up an answer. \
            Use three sentences maximum. Keep the answer as concise as possible. Always say \
            "thanks for asking!" at the end of the answer.  {context} \
            Question: {question}
            Helpful Answer:"""
        QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "query2"],template=template,)
        qa_chain = RetrievalQA.from_chain_type(llm,
                                                retriever=vector_db.as_retriever(),
                                                return_source_documents=True,
                                                chain_type_kwargs={"prompt": QA_CHAIN_PROMPT})

# Streamlit app
def main():
    st.title("Change Interaction AI")
    part1()
    part2()

if __name__ == "__main__":
    main()
